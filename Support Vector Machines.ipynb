{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections there is a code used commonly in the previous exercises to generate the dataset and to define explanatory and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = {'A': 30, 'B': 20}\n",
    "cov = np.array([[4, 2], [2, 4]])\n",
    "means = {'A': np.array([-1, -1]), 'B': np.array([2, 2])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(index=range(sum(n.values())), columns=['x', 'y', 'label'])\n",
    "data.loc[:n['A']-1, ['x', 'y']] = np.random.multivariate_normal(means['A'], cov, n['A'])\n",
    "data.loc[:n['A']-1, ['label']] = 'A'\n",
    "data.loc[n['A']:, ['x', 'y']] = np.random.multivariate_normal(means['B'], cov, n['B'])\n",
    "data.loc[n['A']:, ['label']] = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['x', 'y']]\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check a performance of the Support Vector Machines classifier with linear kernel and built in L2 regularization. Regularization is a technique used to avoid an overfitting by adding some kind of an information. The most frequent methods are L1 and L2 regularization. What makes a difference between them is the penalty term. For SVM classifier we can define the regularization parameter $C$ which serves as a degree of importance that is given to miss-classifications. Intuitively when $C$ grows larger the less wrongly classified examples are allowed. When $C$ tends to 0 more of the miss-classifications are allowed. We will apply some values of the regularization parameter into the model and check when the classifier has the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01 --> Accuracy: 0.6799999999999999\n",
      "C: 0.05 --> Accuracy: 0.76\n",
      "C: 0.1 --> Accuracy: 0.74\n",
      "C: 0.2 --> Accuracy: 0.68\n",
      "C: 0.5 --> Accuracy: 0.7\n",
      "C: 1 --> Accuracy: 0.72\n",
      "C: 2 --> Accuracy: 0.72\n",
      "C: 5 --> Accuracy: 0.7\n",
      "C: 10 --> Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Cs = [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "for C in Cs:\n",
    "    clf = svm.SVC(kernel='linear', C=C)\n",
    "    clf.fit(X, y)\n",
    "    accuracy = cross_val_score(clf, X, y, cv=10).mean()\n",
    "    print('C: {} --> Accuracy: {}'.format(C, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can conclude that regularization parameter of $C = 0.05$ makes the classifier to have the superior accuracy over the other parameters. Now we will check a performance of the LDA classifier and compare it to the SVM method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X, y)\n",
    "accuracy = cross_val_score(clf, X, y, cv=10).mean()\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy of LDA method does not differ significantly from the accuracy obtained by SVM classifier, there are a lot of differences between those methods. First of all SVM classification is an optimization problem while LDA has an analytical solution. LDA makes a use of the entire dataset to estimate the covariance matrices therefore it is sensitive to outliers. On the other hand SVM is optimized over a subset of the data only. Also SVM makes no assumptions about the data (in LDA data needs to be normally distributed) meaning it's more flexible method but it comes at a price of an interpretability. LDA is a linear classifier while SVM can make a use of kernel to change the classifier from linear to non-linear."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
