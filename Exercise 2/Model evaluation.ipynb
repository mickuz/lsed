{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start up by creating a dataframe from the data imported from the given website. Column names were extracted from the file with a describtion of the dataset. We can give a look at our data by displaying first five rows with `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalanity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalanity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "df = pd.read_csv(url, delimiter=',', names=[\n",
    "    'Class',\n",
    "    'Alcohol',\n",
    "    'Malic acid',\n",
    "    'Ash',\n",
    "    'Alcalanity of ash',\n",
    "    'Magnesium',\n",
    "    'Total phenols',\n",
    "    'Flavanoids',\n",
    "    'Nonflavanoid phenols',\n",
    "    'Proanthocyanins',\n",
    "    'Color intensity',\n",
    "    'Hue',\n",
    "    'OD280/OD315 of diluted wines',\n",
    "    'Proline'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define predictor and target variables by selecting appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the task and modularize the code we will define three functions. The first function fits a classifying model (taken as an argument) to the training dataset and predicts the target variable based on test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clf(classifier, X_train, X_test, y_train):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function prints classification metrics comparing predicted output with the test target variable. We chose accuracy, precision and recall to check a performance of the classifier. Accuracy is a most common evaluation metric – it's just the number of correct predictions made as a ratio of all predictions made:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}.\n",
    "\\end{equation}\n",
    "\n",
    "Precision measures the portion of positive identifications in a classification set that were actually correct:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}.\n",
    "\\end{equation}\n",
    "\n",
    "On the other hand recall represents the proportion of actual positives that were identified correctly:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{TP}{TP + FN},\n",
    "\\end{equation}\n",
    "\n",
    "where $TP$ is the number of true positives, $TN$ – true negatives, $FP$ – false positives and $FN$ – false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "def print_metrics(name, classifier, X_train, X_test, y_train, y_test):\n",
    "    y_pred = predict_clf(classifier, X_train, X_test, y_train)\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print('Classifier: {}'.format(name))\n",
    "    print('Accuracy: {}%'.format(str(round(accuracy*100, 2))))\n",
    "    print('Precision: {}%'.format(str(round(precision*100, 2))))\n",
    "    print('Recall: {}%'.format(str(round(recall*100, 2))))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the third function prints and compares the metrics of all the classifiers in a list taken as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(names, classifiers, X_train, X_test, y_train, y_test):\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        print_metrics(name, clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before evaluation of the models is creating lists with names of the classifiers and and the actual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = ['LDA', 'QDA', 'NB']\n",
    "classifiers = [LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), GaussianNB()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the features – training dataset as test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LDA\n",
      "Accuracy: 100.0%\n",
      "Precision: 100.0%\n",
      "Recall: 100.0%\n",
      "------------------\n",
      "Classifier: QDA\n",
      "Accuracy: 99.44%\n",
      "Precision: 99.44%\n",
      "Recall: 99.53%\n",
      "------------------\n",
      "Classifier: NB\n",
      "Accuracy: 98.88%\n",
      "Precision: 98.85%\n",
      "Recall: 98.97%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "compare_models(names, classifiers, X, X, y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the classifiers have almost perfect accuracy it's not a desirable method of model evaluation. Taking every feature into consideration and testing the classifier with training dataset lead to overfitting of our model. It means that the model doesn't generalize well and would have much lower accuracy when testing with independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 2/5/10 features – training dataset as test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LDA\n",
      "Accuracy: 80.9%\n",
      "Precision: 79.87%\n",
      "Recall: 79.67%\n",
      "------------------\n",
      "Classifier: QDA\n",
      "Accuracy: 81.46%\n",
      "Precision: 80.43%\n",
      "Recall: 80.01%\n",
      "------------------\n",
      "Classifier: NB\n",
      "Accuracy: 80.9%\n",
      "Precision: 79.76%\n",
      "Recall: 79.45%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "X_two_features = df.iloc[:, 1:3].values\n",
    "compare_models(names, classifiers, X_two_features, X_two_features, y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LDA\n",
      "Accuracy: 87.64%\n",
      "Precision: 87.13%\n",
      "Recall: 86.72%\n",
      "------------------\n",
      "Classifier: QDA\n",
      "Accuracy: 88.76%\n",
      "Precision: 88.31%\n",
      "Recall: 88.24%\n",
      "------------------\n",
      "Classifier: NB\n",
      "Accuracy: 85.39%\n",
      "Precision: 84.96%\n",
      "Recall: 84.88%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "X_five_features = df.iloc[:, 1:6]\n",
    "compare_models(names, classifiers, X_five_features, X_five_features, y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LDA\n",
      "Accuracy: 98.88%\n",
      "Precision: 98.97%\n",
      "Recall: 98.84%\n",
      "------------------\n",
      "Classifier: QDA\n",
      "Accuracy: 99.44%\n",
      "Precision: 99.44%\n",
      "Recall: 99.53%\n",
      "------------------\n",
      "Classifier: NB\n",
      "Accuracy: 96.07%\n",
      "Precision: 96.3%\n",
      "Recall: 96.2%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "X_ten_features = df.iloc[:, 1:11]\n",
    "compare_models(names, classifiers, X_ten_features, X_ten_features, y, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the models have worse accuracy with decreasing number of features. However they generalize much better. Optimally we should have applied feature selection to choose the features that are the most impactful or correlate with the target variable the most. To do so we could have used regularization techniques or XGBoost algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 2 features – splitting the data into training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LDA\n",
      "Accuracy: 77.27%\n",
      "Precision: 79.45%\n",
      "Recall: 76.49%\n",
      "------------------\n",
      "Classifier: QDA\n",
      "Accuracy: 79.55%\n",
      "Precision: 79.44%\n",
      "Recall: 78.87%\n",
      "------------------\n",
      "Classifier: NB\n",
      "Accuracy: 79.55%\n",
      "Precision: 79.44%\n",
      "Recall: 78.87%\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_two_features, y, test_size=0.25)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33)\n",
    "\n",
    "compare_models(names, classifiers, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training, validation and test datasets provides enhanced generalization because the validation set can be employed to tuning of model's hyperparameters without overfitting. However this method has one major drawback – performance of the model varies greatly with a way the data is split. Changing which observations happens to be in test dataset can significantly change test accuracy. That's where cross validation comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 2 features – cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a techique widely used to test an effectiveness of the model when there is a limited amount of data. Basically it splits the dataset into $k$ folds. One of the folds becomes test dataset and the remaining ones are used for training. The process is being repeated $k$ times with a change of partition employed for testing every time. At the end we can use average of the metrics to check performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.07%\n",
      "Precision: 80.12%\n",
      "Recall: 78.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "lda = classifiers[0]\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall_macro',\n",
    "           'precision': 'precision_macro'}\n",
    "cross_val_scores = cross_validate(lda, X_two_features, y, cv=kfold, scoring=scoring)\n",
    "cross_val_scores = {key: score.mean() for key, score in cross_val_scores.items()}\n",
    "\n",
    "print('Accuracy: {}%'.format(str(round(cross_val_scores['test_accuracy']*100, 2))))\n",
    "print('Precision: {}%'.format(str(round(cross_val_scores['test_precision']*100, 2))))\n",
    "print('Recall: {}%'.format(str(round(cross_val_scores['test_recall']*100, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics of our model have been improved slightly in comparison with splitting technique. We could have increased the accuracy a little if we had applied nonlinear model instead of LDA. For example we can see QDA outperforms LDA in almost every case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
