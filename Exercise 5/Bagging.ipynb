{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use the Iris dataset which is included in *scikit-learn* module. The first step is loading the data with `load_iris()` function and importing the data into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   class  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is an ensemble technique that combines the predictions from multiple models together to make more accurate predictions than any individual model. It's used to reduce the variance for algorithms characterized by the high variance such as Decision Trees. Here we are implementing our own bagging algorithm. The key point of this method is to draw a sample of data which will be used for training the given model and predicting probabilities of the classes. The whole process is repeated for a certain number of times and then the predictions are getting averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def own_bagging(clf, n_estimators, X_train, y_train, X_test):\n",
    "    data = X_train.join(y_train)\n",
    "    agg_probabilities = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        subsample = data.sample(n=random.randint(1, data.shape[0]), replace=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        probabilities = clf.predict_proba(X_test)\n",
    "        agg_probabilities.append(probabilities)\n",
    "    \n",
    "    agg_probabilities = np.mean(agg_probabilities, axis=0)\n",
    "    return agg_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before an evaluation of the implemented algorithm we will create two helper functions. The first of them chooses the class having the highest probability and returns a list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(agg_probabilities):\n",
    "    predictions = [np.argmax(probabilities) for probabilities in agg_probabilities]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function just simply provides the number of false predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def false_predictions(y_test, y_pred):\n",
    "    falses = 0\n",
    "    for pred_value, test_value in zip(y_pred, y_test):\n",
    "        if pred_value != test_value:\n",
    "            falses += 1\n",
    "    return falses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will select the features and target variable and split our data into training and testing datasets with 4:1 proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are able to apply our implementation for LDA and Decision Tree algorithms to spot the differences. We will investigate the number of false predictions for both models with number of data subsamples varying from 1 to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False predictions for LDA:\n",
      "1 estimators: 1\n",
      "2 estimators: 1\n",
      "5 estimators: 1\n",
      "10 estimators: 1\n",
      "20 estimators: 1\n",
      "50 estimators: 1\n",
      "------------------\n",
      "\n",
      "False predictions for Decision Tree:\n",
      "1 estimators: 3\n",
      "2 estimators: 2\n",
      "5 estimators: 2\n",
      "10 estimators: 2\n",
      "20 estimators: 2\n",
      "50 estimators: 2\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "classifiers = [LinearDiscriminantAnalysis(), DecisionTreeClassifier()]\n",
    "names = ['LDA', 'Decision Tree']\n",
    "n_estimator = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print('False predictions for {}:'.format(name))\n",
    "    \n",
    "    for n in n_estimator:\n",
    "        agg_prob = own_bagging(clf, n, X_train, y_train, X_test)\n",
    "        y_pred = predict_class(agg_prob)\n",
    "        print('{} estimators: {}'.format(n, false_predictions(y_test, y_pred)))\n",
    "    \n",
    "    print('------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of LDA number of false predictions remained the same while in case of Decision Tree it has been decreased slightly. That's because LDA is the method having low variance of results thus bagging doesn't have any impact. The differences would have been more visible if we had used more complicated dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
